<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme content="hugo-academic-group"><script src=https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js></script><script src=https://research.zalando.com//js/hugo-academic-group.js></script><link rel=stylesheet href=https://research.zalando.com//css/bootstrap.min.css><script src=https://research.zalando.com//js/bootstrap.min.js></script><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/styles/default.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/highlight.min.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/github-fork-ribbon-css/0.2.2/gh-fork-ribbon.min.css><script src=https://research.zalando.com//js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://research.zalando.com//css/font-awesome.min.css><link rel=stylesheet href=https://research.zalando.com//css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lato:100,300,400,700|Merriweather:100,400,700|Roboto+Mono"><link rel=stylesheet href=https://research.zalando.com//css/hugo-academic-group.css><link rel="shortcut icon" href=https://research.zalando.com//img/favicon.ico type=image/x-icon><link rel=canonical href=https://research.zalando.com/project/sample_efficient_reinforcement_learning/sample_efficient_reinforcement_learning/><title>Sample Efficient Reinforcement Learning | Zalando Research</title></head><body><div class=home-anchor id=home></div></body></html><nav class="navbar navbar-default navbar-fixed-top" id=navbar-main><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=.navbar-collapse aria-expanded=false>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button><div class=navbar-brand><a class=logo href=https://research.zalando.com/><img src=https://research.zalando.com/img/zalando-research-logo.svg alt="University logo"></img></a></div></div><div class="collapse navbar-collapse" id=#navbar-collapse-1><ul class="nav navbar-nav navbar-right"><li class=nav-item><a data-scroll href=https://research.zalando.com/#top>Home</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#members>Members</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#publications>Publications</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#projects>Projects</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#posts>Blog</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#open-projects>Vacancies</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#contact>Contact</a></li></ul></div></div></nav><div class=container><article class="article article-project" itemscope itemtype=http://schema.org/Article><h1 itemprop=name>Sample Efficient Reinforcement Learning</h1><div class=article-style itemprop=articleBody><h3 id=research-project-by-calvin-sewardmembercalvin_s-kashif-rasulmemberkashif_r-ingmar-schustermemberingmar_s--roland-vollgrafmemberroland_v>Research Project by <a href=https://research.zalando.com/member/calvin_s>Calvin Seward</a>, <a href=https://research.zalando.com/member/kashif_r>Kashif Rasul</a>, <a href=https://research.zalando.com/member/ingmar_s>Ingmar Schuster</a> & <a href=https://research.zalando.com/member/roland_v>Roland Vollgraf</a></h3><p><img src=https://research.zalando.com/project/sample_efficient_reinforcement_learning/img/robot.png alt></p><p>Reinforcement learning has seen many successes in the last years, enabling
computers to beat human masters at such difficult tasks as Go and Atari games.
In addition, reinforcement learning is showing great promise in creating
exciting consumer facing technologies such as self driving cars, while also
driving behind the scenes efficiency gains with self learning recommendation
and pricing systems.</p><p>Yet despite all these gains there are major challenges still out there.
For one we have the issue of sample efficiency, the alphaGo computer had
to play 5.4 * 10^9 games to train. Even if Jesus after his resurrection
had set his mind to learning Go and played the game at an ungodly rate of
5 matches a minute ever since, he still wouldn’t have played as many games
as the AlphaGo computer during training. So clearly state-of-the-art
reinforcement learning methods are able to beat human masters, they must
play many more games than the human master practiced with. The Go master
still beats the computer in sample efficiency.</p><p>This issue becomes more pronounced when samples are expensive. One example
is robot control: there a sample is obtained by letting the robot move around,
which can take a few minutes to gather a sample. Clearly if we’re in the regime
where hundreds of billions of samples are required, the robot will probably not
be trained before the four horsemen of the apocalypse show up and ruin the experiment.</p><p>One reason sample efficiency is so difficult is because these are all delayed
reward environments where the reward becomes apparent only after a bunch of
interactions with the environment. With Q learning, it’s been shown that reward
delay exponentially increases the time it takes to reduce the bias of the learned
Q-function. In order to address this issue, many approaches craft an auxiliary
reward structures for the agent. For example, in chess the agent can gain reward
by capturing opposing pieces, lose if by having his pieces captured. In this way
the agent learns the intermediate goal of capturing opposing pieces which is
generally necessary to achieve the actual goal: checkmate.</p><p>While the approach gets good results in specific domains, for every new domain
the agent wishes to learn a new reward structure must be crafted by hand, and in
many cases not enough knowledge exists about the domain to hand craft such a good
reward structure. In addition, this puts an unnecessary upper limit on the
performance of the whole system by introducing a target which is not aligned
with the (generally unknown) best policy.</p><p>Therefore, in order to better understand this problem we’re researching bounds
on sample efficiency in simple, delayed reward settings, both minimax lower
bounds for all methods and concrete upper bounds for popular methods. With these
bounds in hand, we can understand the weaknesses in current methods and hopefully
find new methods that get close to lower bounds.</p></div></article><nav><ul class=pager><li class=previous><a href=https://research.zalando.com/project/adversarial_learning/adversarial_learning/><span aria-hidden=true class=darknav>&larr;&nbsp;Previous project:</span>
Adversarial Learning</a></li><li class=next><a href=https://research.zalando.com/project/fashion_dna/fashion_dna/><span class=darknav>&nbsp;Next project:</span>
Fashion DNA
<span aria-hidden=true class=darknav>&rarr;</span></a></li></ul></nav></div><footer class=site-footer><div class=container><p class=powered-by><a href=https://research.zalando.com/imprint>Imprint</a> · <a href=https://research.zalando.com/privacy>Privacy Policy</a> · © Zalando SE, 2021 &#183;
Partially powered by the <a href=https://github.com/gcushen/hugo-academic target=_blank>Academic theme</a> for <a href=http://gohugo.io target=_blank>Hugo</a>.
<span class=pull-right><a href=#home id=back_to_top><span class=button_icon><i class="fa fa-chevron-up fa-2x" aria-hidden=true></i></span></a></span></p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>