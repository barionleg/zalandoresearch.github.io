<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme content="hugo-academic-group"><meta http-equiv=refresh content="0;url=https://github.com/zalandoresearch/"><script src=https://research.zalando.com//js/jquery.min.js></script>
<script src=https://research.zalando.com//js/hugo-academic-group.js></script>
<link rel=stylesheet href=https://research.zalando.com//css/bootstrap.min.css><script src=https://research.zalando.com//js/bootstrap.min.js></script>
<link rel=stylesheet href=https://research.zalando.com//css/default.min.css><script src=https://research.zalando.com//js/highlight.min.js></script>
<link rel=stylesheet href=https://research.zalando.com//css/gh-fork-ribbon.min.css><script src=https://research.zalando.com//js/highlight.pack.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://research.zalando.com//css/font-awesome.min.css><link rel=stylesheet href=https://research.zalando.com//css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lato:100,300,400,700|Merriweather:100,400,700|Roboto+Mono"><link rel=stylesheet href=https://research.zalando.com//css/hugo-academic-group.css><link rel="shortcut icon" href=https://research.zalando.com//img/favicon.ico type=image/x-icon><link rel=canonical href=https://research.zalando.com/publication/bandit_2018/><title>A Bandit Framework for Optimal Selection of Reinforcement Learning Agents | Zalando Research</title><style>body *{display:none}body #redirect{display:block;margin:1em;font-weight:700}body #redirect a{display:inline}</style></head><body><div id=redirect>Redirecting to <a href=https://github.com/zalandoresearch/>github.com/zalandoresearch/</a> ...</div><div class=home-anchor id=home></div></body></html><nav class="navbar navbar-default navbar-fixed-top" id=navbar-main><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=.navbar-collapse aria-expanded=false>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button><div class=navbar-brand><a class=logo href=https://research.zalando.com/><img src=https://research.zalando.com/img/zalando-research-logo.svg alt="University logo"></img></a></div></div><div class="collapse navbar-collapse" id=#navbar-collapse-1><ul class="nav navbar-nav navbar-right"><li class=nav-item><a data-scroll href=https://research.zalando.com/#top>Home</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#members>Members</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#publications>Publications</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#projects>Projects</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#posts>Blog</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#open-projects>Vacancies</a></li><li class=nav-item><a data-scroll href=https://research.zalando.com/#contact>Contact</a></li></ul></div></div></nav><div class=container><div class=pub itemscope itemtype=http://schema.org/CreativeWork><div class=row><div class=col-sm-12><div class=pub-title><h1 itemprop=name>A Bandit Framework for Optimal Selection of Reinforcement Learning Agents</h1></div></div></div><div class=row><div class=col-sm-12><div class=pub-authors itemprop=author><div itemprop=author><span class=author-name>Andreas Merentitis
</span>,
<span class=author-name><a href=https://research.zalando.com/alumni/kashif_r>Kashif Rasul</a>
</span>,
<span class=author-name><a href=https://research.zalando.com/alumni/roland_v/>Roland Vollgraf</a>
</span>,
<span class=author-name>Abdul-Saboor Sheikh
</span>,
<span class=author-name>Urs Bergmann</span></div></div></div></div><div class=row><div class=col-sm-12><h3>Abstract</h3></div></div><div class=row><div class=col-sm-12><p class=pub-abstract itemprop=text>Deep Reinforcement Learning has been shown to be very successful in complex games, e.g. Atari or Go. These games have clearly defined rules, and hence allow simulation. In many practical applications, however, interactions with the environment are costly and a good simulator of the environment is not available. Further, as environments differ by application, the optimal inductive bias (architecture, hyperparameters, etc.) of a reinforcement agent depends on the application. In this work, we propose a multi-arm bandit framework that selects from a set of different reinforcement learning agents to choose the one with the best inductive bias. To alleviate the problem of sparse rewards, the reinforcement learning agents are augmented with surrogate rewards. This helps the bandit framework to select the best agents early, since these rewards are smoother and less sparse than the environment reward. The bandit has the double objective of maximizing the reward while the agents are learning and selecting the best agent after a finite number of learning steps. Our experimental results on standard environments show that the proposed framework is able to consistently select the optimal agent after a finite number of steps, while collecting more cumulative reward compared to selecting a sub-optimal architecture or uniformly alternating between different agents.</p></div></div><div class=row><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading">Publication</div><div class="col-xs-12 col-sm-9">32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montreal, Canada. <a href=https://sites.google.com/view/deep-rl-workshop-nips-2018/home>Deep Reinforcement Learning Workshop</a></div></div></div></div><div class="visible-xs space-below"></div><div class=row><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading">Date</div><div class="col-xs-12 col-sm-9" itemprop=datePublished>December, 2018</div></div></div></div><div class="visible-xs space-below"></div><div class=row style=padding-top:10px><div class=col-sm-12><div class=row><div class="col-xs-12 col-sm-3 pub-row-heading" style=line-height:34px>Links</div><div class="col-xs-12 col-sm-9"><a class="btn btn-primary btn-outline" href=https://arxiv.org/pdf/1902.03657.pdf>PDF</a></div></div></div></div><div class="visible-xs space-below"></div><div class=space-below></div><div class=article-style></div></div></div><footer class=site-footer><div class=container><p class=powered-by><a href=https://research.zalando.com/imprint>Imprint</a> · <a href=https://en.zalando.de/zalando-privacy-policy/>Privacy Policy</a> · © Zalando SE, 2021 &#183;
Partially powered by the <a href=https://github.com/gcushen/hugo-academic target=_blank>Academic theme</a> for <a href=http://gohugo.io target=_blank>Hugo</a>.
<span class=pull-right><a href=#home id=back_to_top><span class=button_icon><i class="fa fa-chevron-up fa-2x" aria-hidden=true></i></span></a></span></p></div></footer><script src=https://research.zalando.com/js/TweenMax.min.js></script>
<script src=https://research.zalando.com/js/ScrollToPlugin.min.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://research.zalando.com/js/tex-mml-chtml.js></script></body></html>